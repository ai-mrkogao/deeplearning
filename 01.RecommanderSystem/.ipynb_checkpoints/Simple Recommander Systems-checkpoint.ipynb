{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Problems with Average Rating & Explore vs. Exploit(Part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More problems with average rating\n",
    "- What if N is very small or 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\bar{X}= \\frac{1}{N}\\sum_{i=1}^{N} X_i \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing (or dampening)\n",
    "- we've seen this in the context of NLP for word probabilities, also in PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "r= \\frac{\\sum_{i=1}^{N} X_i + \\lambda\\mu_0}{N+\\lambda}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- could make $\\mu_0$ the global average, or just some middle value like 3(3 Star value of 5 Star score)\n",
    "- 1000 4 star ratings $\\mu_0$ = 3, $\\lambda$ = 1 -> 3.999\n",
    "- 5 4 star ratings -> 3.83\n",
    "- one 4 star ratings -> 3.5\n",
    "- We'll see how bayesian approach yields same formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore- Exploit Dilemma \n",
    "- we've discussed before in context of A/B testing and Reinforcement Learning, in case you want to learn more\n",
    "- You are playing slot machines at casino\n",
    "- All machines look ths same, so you have to play them to determine which is best  \n",
    "- suppose only 2 outcomes: win(1), lose(0)\n",
    "- best machine has highest win rate \n",
    "\\begin{equation*}\n",
    "\\hat{p}= p(win) = \\frac{wins}{total}\n",
    "\\end{equation*}\n",
    "\n",
    "- How many times should I play each machine?\n",
    "- Too few times-> estimate is no good\n",
    "    - so collecting more data is good\n",
    "- win once out of 3 -> p(win) = 1/3\n",
    "    - fat confidence interval\n",
    "- traditional statistical tests can tell us whether or not there's a significant difference between win rates and between machines\n",
    "- Play 10 different machines 100 times each = 1000 turns\n",
    "- 9 machines are suboptimal-> 900 turns yielded a suboptimal reward\n",
    "- Hence we have a dilemma, play more? or play less?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore-Exploit in Recommenders\n",
    "- Ex. You watch a bunch of YouTube videos on how to make eggs\n",
    "- Now your recommendations are filled with videos about making eggs\n",
    "- Probably suboptimal- once I've figured out how to make eggs, I don't want to watch more eggs videos\n",
    "- Should there be a stronger exploration component?\n",
    "- Maybe I'd like to see movie trailers or machine learning videos\n",
    "- But you could just as easily explore with knitting videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore- Exploit\n",
    "- How do we strike a balance between these 2 opposing forces?\n",
    "- Smoothed average gives us one part of the solution\n",
    "    - But it's fine if you just want to do something simple\n",
    "- Next: The Bayesian solution to this problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Bayesian Approach Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach to the Explore-Exploit Dilemma\n",
    "- Main theme: Everything is a random variable\n",
    "- Random variables have a probability distribution, parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Example\n",
    "- Binary outcome\n",
    "- PMF(probability mess function) of x is given below\n",
    "- In the Bayesian paradigm, $\\pi$ is also a RV(random variable), what is distribution?\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x)= \\pi^{x}(1-\\pi)^{1-x}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distribution of $\\pi$\n",
    "\n",
    "- $\\pi$ has a Beta distribution\n",
    "- x is a discrete Random variable, but $\\pi$ is a continuous random variable\n",
    "- $\\pi$ is in the range [0,1]\n",
    "- x only has one parameter: $\\pi$\n",
    "- $\\pi$ has 2 parameters: $\\alpha,\\beta$\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi) = \\frac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} , B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n",
    "\\end{equation*}\n",
    "\n",
    "[beta distribution wikipedia](https://en.wikipedia.org/wiki/Beta_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian approach\n",
    "- Non-Bayesian (frequentist)\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\prod_{i=1}^{N}\\pi^{x_{i}}(1-\\pi)^{1-x_{i}} ,(likelihood) \\\\\n",
    "\\hat{\\pi} = argmax_{\\pi}(L) = \\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "- Bayesian\n",
    "    - There is no $\\hat{\\pi}$ \n",
    "    - Instead, we want p($\\pi | x_{1},...,x_{N}) = p(\\pi |X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just use Bayers rule\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\frac{p(X|\\pi)p(\\pi)}{\\int_{0}^{1}{p(X|\\pi)p(\\pi)d\\pi}}\n",
    "\\end{equation*}\n",
    "\n",
    "- In general, posterior = likelihood * prior / normalizing constant\n",
    "- we can choose p($\\pi$) = Beta(1,1) which is the uniform dist\n",
    "- Why make this so complicated? integral are hard\n",
    "- In general, they are usually infeasible to calculate(but not in our case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change it to a proportionality\n",
    "- Drop the normalizing constant\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\propto{p(X|\\pi)p(\\pi)}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug in the things we konw\n",
    "\n",
    "- we know the expression for likelihood and prior, let's just plug them in\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\propto[\\prod_{i=1}^{N}\\pi^{x_{i}}(1-\\pi)^{1-x_{i}}]\\frac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n",
    "p(\\pi|X) = \\propto{\\pi^{\\alpha+(\\sum_{i=1}^{N}X_{i})-1}}(1-\\pi)^{\\beta+N-{\\sum_{i=1}^{N}X_{i}}-1}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we found?\n",
    "- The posterior p($\\pi$|X) = is simply another Beta distribution\n",
    "- Posterior\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = const \\times [{\\pi^{\\alpha+(\\sum_{i=1}^{N}X_{i})-1}}(1-\\pi)^{\\beta+N-{\\sum_{i=1}^{N}X_{i}}-1}] \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- Prior\n",
    "\\begin{equation*}\n",
    "p(\\pi) = const \\times \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\n",
    "\\end{equation*}\n",
    "\n",
    "- We have discovered\n",
    "\\begin{equation*}\n",
    "\\pi|X \\sim Beta(\\alpha',\\beta'), \\alpha' = \\alpha+\\sum_{i=1}^{N}X_{i},\\beta' = \\beta+N -\\sum_{i=1}^{N}X_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Remark\n",
    "\n",
    "- From the formula, it appears we have collected N samples of X, and we update the posterior after having collected all of them\n",
    "- But we can update the posterior after collecting each sample\n",
    "- The posterior of one step becomes the prior of the next step\n",
    "- Online Learning\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha' = \\alpha+\\sum_{i=1}^{N}X_{i},\\beta' = \\beta+N -\\sum_{i=1}^{N}X_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "1. $\\pi \\sim Beta(1,1)$\n",
    "2. Sample $x_{1}$\n",
    "3. $\\pi \\sim Beta(1+x_{1},1+1-x_{1})$\n",
    "4. Sample $x_{2}$\n",
    "5. $\\pi \\sim Beta(1+x_{1}+x_{2},1+2-x_{1}-x_{2})$\n",
    "6. etc... in general: $\\alpha' = \\alpha + x_{i},\\beta' = \\beta +1 - x+{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Priors\n",
    "\n",
    "- In general, this integral is hard to calculate, except in special circumstances(like the one we just looked at) \n",
    "- Special pairs of likelihood and prior, such that the posterior has same type of distribution as prior \n",
    "- It's the exception rather than the rule \n",
    "- Check out the wikipedia page on conjugate priors for a list \n",
    "\n",
    " \\begin{equation*}\n",
    "p(\\pi|X) = \\frac{p(X|\\pi)p(\\pi)}{\\int_{0}^{1}{p(X|\\pi)p(\\pi)d\\pi}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Bayesian Approach Part2(Sampling and ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How does that help us with ranking?\n",
    "\n",
    "- The key trick is to sample from the posterior\n",
    "- Every other technique will use a deterministic ranking but not this one\n",
    "- The Bayesian method is to sort by the samples drawn from the posteriors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario #1\n",
    "- 3 slot machines with 3 different means, skinny confidence intervals\n",
    "- clearly, any samples drawn would result in green>orange> blue\n",
    "- The chance of that not being the case is vanishingly small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario #2\n",
    "- One skinny (distribution), one fat(distribution)\n",
    "- Fat distribution has higher variance(we need more samples)\n",
    "- p(1 > x > 0. 55) is larger for the blue curve\n",
    "- p(0< x< 0.45) is larger for the blue curve\n",
    "- sampleing automatically balances between exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "- Key is to rank by samples drawn from posteriors\n",
    "    - Each item has its own posterior\n",
    "- Automatically balances between explore and exploit\n",
    "- sampling the posterior is intelligently random rather than totally random since it accounts for the data we've collected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Bayesian Approach Part 3 (Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if your likelihood is not Bernoulli?\n",
    "- A Bernoulli likelihood means your data can only be 0 or 1\n",
    "- A Gaussian can be any real number\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x)= \\pi^{x}(1-\\pi)^{1-x}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Generalize the Bayesian approach\n",
    " - Start with some likelihood\n",
    " \n",
    " \\begin{equation*}\n",
    "p(X|\\theta)= \\prod_{i=1}^{N}p(X_{i}|\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "- Choose a prior\n",
    "    - including hyperparameters $P(\\theta)$\n",
    "    - $\\theta$ is random variable \n",
    "- Calculate the posterior\n",
    "$p(\\theta|X) \\propto p(X|\\theta)p(\\theta)$\n",
    "- Draw samples from the posterior for ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian likelihood\n",
    "- Bernoulli distribution has 1 parameter(p) , but Gaussian has 2(mean and variance)\n",
    "- Should you place a prior on mean, variance, or both?\n",
    "- All 3 situations are possible\n",
    "\n",
    "\n",
    " \\begin{equation*}\n",
    "p(X|\\mu,\\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gaussian likelihood\n",
    " \n",
    " - In Bayesian statistics, it's more convenient to work with precision(inverse variance)\n",
    "    - 1) Model p($\\mu$), $\\tau$ known\n",
    "        - p($\\mu$) $\\sim$Normal\n",
    "    - 2) Model $p(\\tau)$, $\\mu$ known\n",
    "        - $p(\\tau)$ $\\sim$ Gamma\n",
    "    - 3) Model p($\\mu,\\tau$)\n",
    "        - $p(\\mu,\\tau) \\sim Normal-Gamma$\n",
    "\n",
    "\\begin{equation*}\n",
    "p(X|\\mu,\\tau)= \\sqrt{\\frac{\\tau}{2\\pi}}exp(-\\frac{\\tau(x-\\mu)^2}{2})\n",
    "\\end{equation*}\n",
    "\n",
    "[inverse variance](https://en.wikipedia.org/wiki/Inverse-variance_weighting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Conjugate Prior\n",
    "- See Appendix for full derivation\n",
    "- Likelihood \n",
    "\\begin{equation*}\n",
    "X \\sim N(\\mu,\\tau^{-1})\n",
    "\\end{equation*}\n",
    "\n",
    "- Prior  \n",
    "$\\mu$ is random variable\n",
    "\\begin{equation*}\n",
    "\\mu \\sim N(m,\\lambda^{-1})\n",
    "\\end{equation*}\n",
    "\n",
    "- Posterior update\n",
    "\\begin{equation*}\n",
    "m' = \\frac{\\lambda m+\\tau\\sum_{i=1}^{N}X_{i}}{\\lambda+N\\tau} \\\\\n",
    "\\lambda' = \\lambda + N\\tau\n",
    "\\end{equation*}\n",
    "\n",
    "- As N increases, $\\lambda' -> \\infty, \\sigma^2 -> 0$\n",
    "- if N =0 , mean of $\\mu$ is m - prior mean\n",
    "- As N increases, $\\mu$ approaches sample mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection with smoothed average\n",
    "\n",
    "- Smoothed Average:\n",
    "\\begin{equation*}\n",
    "r = \\frac{\\lambda \\mu_{0}+\\sum_{i=1}^{N}X_{i}}{\\lambda+N} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- Posterior mean:\n",
    "\\begin{equation*}\n",
    "m' = \\frac{\\lambda m/ \\tau +\\sum_{i=1}^{N}X_{i}}{\\lambda /\\tau+N} \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta posterior mean\n",
    "- In this case, it's more like add-1 smoothing we would use in NLP\n",
    "- (we'll encounter this again when we discuss Markov models)\n",
    "\n",
    "\\begin{equation*}\n",
    "E(\\pi) = \\frac{\\alpha'}{\\alpha'+\\beta'} = \\frac{\\alpha+ (\\sum_{i=1}^{N}X_{i})}{\\alpha+\\beta+N}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Bayesian Approach Part4 (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - slot machine using Bayesian approach\n",
    " - going to rank each bandit according to the samples drawn from posteriors\n",
    " - draw samples according to the Beta distribution \n",
    " - News Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta distribution\n",
    "from scipy.stats import beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target answer about slot machine wins rates\n",
    "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(object):   \n",
    "    def __init__(self, p):\n",
    "        # p is true success rate\n",
    "        # a,b beta parameters\n",
    "        self.p = p\n",
    "        self.a = 1\n",
    "        self.b = 1\n",
    "\n",
    "    def pull(self):\n",
    "        # Bernoulli distribution, 0 or 1\n",
    "        return np.random.random() < self.p\n",
    "\n",
    "    def sample(self):\n",
    "        #sample from beta distribution\n",
    "        return np.random.beta(self.a, self.b)\n",
    "\n",
    "    def update(self, x):\n",
    "        # Online Learning , update each sample\n",
    "        self.a += x\n",
    "        self.b += 1 - x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(bandits, trial):\n",
    "  x = np.linspace(0, 1, 200)\n",
    "  # plot beta pdf\n",
    "  for b in bandits:\n",
    "    y = beta.pdf(x, b.a, b.b)\n",
    "    plt.plot(x, y, label=\"real p: %.4f\" % b.p)\n",
    "  plt.title(\"Bandit distributions after %s trials\" % trial)\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "  bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]\n",
    "\n",
    "  sample_points = [5,10,20,50,100,200,500,1000,1500,1999]\n",
    "  for i in range(NUM_TRIALS):\n",
    "\n",
    "    # take a sample from each bandit\n",
    "    bestb = None\n",
    "    maxsample = -1\n",
    "    allsamples = [] # let's collect these just to print for debugging\n",
    "    for b in bandits:\n",
    "      # drawing a sample from each beta distribution\n",
    "      sample = b.sample()\n",
    "      allsamples.append(\"%.4f\" % sample)\n",
    "      if sample > maxsample:\n",
    "        maxsample = sample\n",
    "        bestb = b\n",
    "    if i in sample_points:\n",
    "      print(\"current samples: %s\" % allsamples)\n",
    "      plot(bandits, i)\n",
    "\n",
    "    # pull the arm for the bandit with the largest sample\n",
    "    x = bestb.pull()\n",
    "\n",
    "    # update the distribution for the bandit whose arm we just pulled\n",
    "    bestb.update(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guassian Implementaion\n",
    "- rl/comparing_explore_exploit_methods.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Demographics and Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Machine Learning\n",
    "- we have some inputs(X) and corresponding targets(Y)\n",
    "- Y might represent\n",
    "    - Did the user buy the product?\n",
    "    - Click on the ad\n",
    "    - Click on the article\n",
    "    - Sign up for the newsletter\n",
    "    - Make an account\n",
    "    - What did the user rate this item\n",
    "- If our model predictions are accurate, then we can use it to recommand items the user is more likely to buy/click/rate highly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Features\n",
    "- Common features include demographics\n",
    "    - Age\n",
    "    - Gender\n",
    "    - Religion\n",
    "    - Location\n",
    "    - Race\n",
    "    - Occupation\n",
    "    - Education level\n",
    "    - Martial status\n",
    "    - Socio economic status\n",
    "- data the goverment collects\n",
    "- Can include any other data collected by your site\n",
    "    - when the user signed up\n",
    "    - which pages they reviewed\n",
    "    - Have credit card history\n",
    "    - purchase history\n",
    "- Can purchase data from other companies\n",
    "    - Acxiom\n",
    "    - intelius\n",
    "- Once you have data, any supervised model will do\n",
    "    - Logistic regression\n",
    "    - random forest\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the item itself?\n",
    "- Given your age and gender, the probability you will buy an iPhone is different than the probability you will buy cat food\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A seperate model for each prodcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add product attributes as model inputs\n",
    "- input layer includes like retina display, RAM and etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've done an example like this before\n",
    "- e-commerce example\n",
    "\n",
    "- Input feature\n",
    "    - is_mobile\n",
    "    - n_products_viewed\n",
    "    - visit_duration\n",
    "    - is_returning_visitor\n",
    "    - time_of_day\n",
    "- Targets\n",
    "    - bounce\n",
    "    - add to cart\n",
    "    - begin checkout\n",
    "    - complete checkout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "- Not easy\n",
    "- you can buy it, not cheap\n",
    "- privacy- ad and tracking blockers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making this priciple more flexible\n",
    "- Instead of explicit features like age, gender, etc. we will learn features implicitly\n",
    "- Latent variable models - features are learned automatically(\"hidden causes\")\n",
    "- May not be interpretable or represent neatly defined concepts like age\n",
    "- we don't have to collect those features manually\n",
    "- (user, item,rating) is enough\n",
    "- will revisit when we talk about matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - PageRank part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google PageRank\n",
    "- used for search Result\n",
    "- ideally you have some knowledge about Markov Models\n",
    "- The Answer: The PageRank of a page is the probability I woudl end up that page if I surfed the internet randomly for an infinite amount of time\n",
    "\n",
    "- Fits into our idea of recommenders: it's just a score and to make our recommendations list we sort items by score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Models\n",
    "- Simplest way to think about Markov Models are bigrams form NLP\n",
    "- Build a probabilistic language model\n",
    "- Can ask \"what is the probability the next word in a sentence is 'love', given the previous word was 'I'?\n",
    "- p(love | I)\n",
    "- p(the | the)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "- It's a bigrams because we only consider 2 words at a time\n",
    "- sentence: \" I love cats\"\n",
    "- We would not model p(cat| I love), just p(cats | love)\n",
    "- Not realistic\n",
    "- What words can come after and?\n",
    "- Too many enumerate\n",
    "- Does this mena Markov Models are bad? No! just results matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Models\n",
    "- we don't have to think of each item as a word, just a generic state: x(t)\n",
    "\n",
    "- \"Markov\" means x(t) doesn't depend on any values 2 or more steps behind - only the immediate last value\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_{t}|x_{t-1},x_{t-2},...,x_{1}) = p(x_{t}|x_{t-1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probability Matrix\n",
    "- A(i,j) tells us the probability of going to state j from state i\n",
    "\n",
    "\\begin{equation*}\n",
    "A(i,j) = p(x_{t} = j | x_{t-1} = i)\n",
    "\\end{equation*}\n",
    "\n",
    "- Key: rows must sum to 1\n",
    "    - Since it's a probability this must be true\n",
    "    - If true, A is called a stochastic matrix or Markov Matrix\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j=1}^{M}A(i,j) = \\sum_{j=1}^{M}p(x_{t} = j | x_{t-1} = i) = 1\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "- State 1 = Sunny\n",
    "- State 2 = Rainy\n",
    "- Let's suppose:\n",
    "    - p(sunny | sunny) = 0.9\n",
    "    - p(sunny | rainy) = 0.1\n",
    "    - p(rainy | sunny) = 0.1\n",
    "    - p(rainy | rainy) = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating probabilities\n",
    "- we saw this concept earlier when discussing problems with average rating\n",
    "- How do we calculate probabilities in a transition matrix?\n",
    "\n",
    "\\begin{equation*}\n",
    "p(rainy | sunny) = \\frac{count(sunny->rainy)}{count(sunny} \\\\\n",
    "p(hate | I) = \\frac{count(I->hate)}{count(I)}\\\\\n",
    "p(love | I) = \\frac{count(I->love)}{count(I)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's the probability of the sentence:\n",
    "    -\"The quick brown fox jumps over the lazy dog?\"\n",
    "\n",
    "\\begin{equation*}\n",
    "p(the)p(quick|the)p(brown|quick) ...\\\\\n",
    "p(x_{1},...,x_{T}) = p(x_{1})\\prod_{t=2}^{T}p(x_{t}|x_{t-1})\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "- Suppose I come across a sentence containing a bigram that didn't appear in my train set\n",
    "- The probability is 0 , Anything *0 = 0\n",
    "\n",
    "\\begin{equation*}\n",
    "p(the)p(quick|the)p(brown|quick) ...\\\\\n",
    "p(x_{1},...,x_{T}) = p(x_{1})\\prod_{t=2}^{T}p(x_{t}|x_{t-1})\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add - 1 Smoothing\n",
    "- Add a \"fake count\" to every possible bigram\n",
    "- V = vocabulary size = number of unique words in dataset\n",
    "- In our case, V = M(number of states), since each state is a word\n",
    "- p(and|and) never occurs but would get positive probability\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_{t} = j|x_{t-1} = i) = \\frac{count(i\\rightarrow j)+1}{count(i) + V}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this remind you of\n",
    "- Beta posterior mean(in this case, only 2 possible outcomes - now V possible outcomes)\n",
    "- is similar to having a Beta(1,1) prior(Dirichlet(1) is more accurate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "E(\\pi) = \\frac{\\alpha'}{\\alpha'+\\beta'} = \\frac{\\alpha+ (\\sum_{i=1}^{N}X_{i})}{\\alpha+\\beta+N} \\\\ \n",
    "p(x_{t} = j|x_{t-1} = i) = \\frac{count(i\\rightarrow j)+1}{count(i) + V}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add - epsilon smoothing\n",
    "- you can add any non-negative number, not just 1\n",
    "- just ensure each row still sums to 1\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_{t} = j|x_{t-1} = i) = \\frac{count(i\\rightarrow j)+\\epsilon}{count(i) + \\epsilon V}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - PageRank part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Distribution\n",
    "\n",
    "\n",
    "$\\pi_{t}$ = state probability distribution at time t\n",
    "\n",
    "- Example: if our 2 states are \"sunny\" and \"rainy\"\n",
    "- Then $\\pi(t)$ will be a vector of size 2\n",
    "- convention that $\\pi(t)$ is a row vector\n",
    "\n",
    "$\\pi_{t} = [p(x_{t} = sunny),p(x_{t}= rainy)]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future State Distribution\n",
    "- Can I calculate $\\pi(t+1)$? Sure, use Bayes rule\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_{t+1} = j) = \\sum_{i=1}^{M} p(x_{t+1} = j,x_{t}=i)\\\\\n",
    "               = \\sum_{i=1}^{M} p(x_{t+1} = j|x_{t}=i)p(x_{t}=i)\\\\\n",
    "               = \\sum_{i=1}^{M} A(i,j)\\pi_{t}(i)\\\\\n",
    "               = \\pi_{t+1}(j)\n",
    "\\end{equation*}\n",
    "\n",
    "- above means the total probability of arriving at j, is the sum of all probabilities considering all different places I could have come from\n",
    "\n",
    "- Since A is a matrix and $\\pi$(t) is a vector, can we express it in terms of matrix math\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{t+1}(j)  = \\sum_{i=1}^{M} A(i,j)\\pi_{t}(i) \\\\\n",
    "\\pi_{t+1}(j)  = \\pi_{t}A\n",
    "\\end{equation*}\n",
    "\n",
    "- What about 2 steps ahead or more?\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{t+2}(j)  = \\pi_{t}AA = \\pi_{t}A^2 \\\\\n",
    "\\pi_{t+k}(j)  = \\pi_{t}A^k\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinity\n",
    "- Why does this make sense? $\\infty$ +1 is still $\\infty$\n",
    "- This is just the eigenvalue problem\n",
    "    - Given a matrix A, find a vector and a scalar. Multiplying the vector by A is equivalent to stretching it by the scalar\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{\\infty} = \\lim\\limits_{t \\to \\infty}\\pi_{0}A^t \\\\\n",
    "\\pi_{\\infty} = \\pi_{\\infty}A \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank\n",
    "- What does any of this crazy math have to do with PageRank?\n",
    "- Every page on the internet is a state in a Markov Model\n",
    "- The \"trainsition probability\" is distributed equally amongst all links on a page\n",
    "\n",
    "- p(dlc.com|lp.me) = 0.5\n",
    "- p(yt.com|lp.me) = 0.5\n",
    "\n",
    "- we can write the transition probability more generally\n",
    "\n",
    "$p(x_{t} = j| x_{t-1}= i) = 1/n(i)$ if i links to j,n(i) = #links on page i 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "- there are billions of pages on the internet, most transition probabilities are 0\n",
    "- we shall add smoothing\n",
    "- Do a simple example on paper to show that if A & U are valid Markov matrices, then so is G\n",
    "\n",
    "- M is total number of states\n",
    "\\begin{equation*}\n",
    "G = 0.85A + 0.15U, U(i,j) = 1/M,   \\forall i,j = 1,...,M\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Distribution\n",
    "- Find the limiting distribution of G - yields a vector of length M- these probabilities are the respective PageRanks for each page on the internet\n",
    "    - A function to find eigenvalues/vectors is np.linalg.eig\n",
    "    - but, this version would have to be scalable for a matrix with billions of rows and cols\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{\\infty} = \\pi_{\\infty}G \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Evaluating a Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Hacker News ranking really works: scoring, controversy, and penalties\n",
    "http://www.righto.com/2013/11/how-hacker-news-ranking-really-works.html\n",
    "\n",
    "The Evolution Of Hacker News\n",
    "https://techcrunch.com/2013/05/18/the-evolution-of-hacker-news/\n",
    "\n",
    "Reddit sorting code\n",
    "https://github.com/reddit-archive/reddit/blob/master/r2/r2/lib/db/_sorts.pyx\n",
    "\n",
    "Revealed: US spy operation that manipulates social media\n",
    "https://www.theguardian.com/technology/2011/mar/17/us-spy-operation-social-networks\n",
    "\n",
    "5G Got me Fired\n",
    "https://medium.com/@dvorak/5g-got-me-fired-ce407e584c4a\n",
    "\n",
    "Learning to rank\n",
    "https://en.wikipedia.org/wiki/Learning_to_rank#Evaluation_measures\n",
    "\n",
    "How Not To Sort By Average Rating\n",
    "https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "\n",
    "Wilson score interval\n",
    "https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\n",
    "\n",
    "redditâ€™s new comment sorting system\n",
    "https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system/\n",
    "\n",
    "Markov Chains Explained Visually\n",
    "http://setosa.io/ev/markov-chains/\n",
    "\n",
    "An algorithmic framework for performing collaborative filtering\n",
    "https://dl.acm.org/citation.cfm?id=312682\n",
    "\n",
    "Item-based collaborative filtering recommendation algorithms\n",
    "https://dl.acm.org/citation.cfm?id=372071\n",
    "\n",
    "FunkSVD\n",
    "http://sifter.org/~simon/journal/20061211.html\n",
    "\n",
    "Probabilistic Matrix Factorization\n",
    "https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "\n",
    "Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo\n",
    "https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "\n",
    "Algorithms for Non-negative Matrix Factorization\n",
    "https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf\n",
    "\n",
    "Learning the parts of objects by non-negative matrix factorization\n",
    "http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf\n",
    "\n",
    "Restricted Boltzmann Machines for Collaborative Filtering\n",
    "https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf\n",
    "\n",
    "AutoRec: Autoencoders Meet Collaborative Filtering\n",
    "http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf\n",
    "\n",
    "Collaborative Filtering for Implicit Feedback Datasets\n",
    "http://yifanhu.net/PUB/cf.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
